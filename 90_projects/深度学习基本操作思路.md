## 深度学习的一般操作思路（5 个 Step）

### Step 1 — 数据向量化（Representation）

若数据本身已经是向量，就直接用该向量；  
若不是向量（图像 / 图 / 视频 / 文本等），先转化为向量或张量作为输入。

本质：所有深度学习模型的输入都必须是 tensor，例如：

- 图像 → 像素张量（如 $x \in \mathbb{R}^{C \times H \times W}$）
- 文本 → token embedding 序列
- 图结构 → 节点 / 边的 embedding
- 视频 → 帧序列的时序 embedding

---

### Step 2 — 数学化任务目标 = 损失函数（Optimization Objective）

把任务写成一个优化问题，通常是最小化损失函数：

- 统一形式：$\min_\theta \ \mathbb{E}[\ell(\theta; x, y)]$
- 分类：交叉熵损失
- 回归：均方误差（MSE）
- 生成：负对数似然 / KL 散度
- 自监督：对比损失、重构误差等
- 强化学习：最大化回报（等价于最小化负回报）

这一步是“把任务变成数学问题”。

---

### Step 3 — 设计模型结构（Architecture / Algorithmic Framework）

为实现 Step 2 的目标，选择一个可微的参数化模型族：

- 判别式：MLP / CNN / RNN / Transformer / GNN 等 $f_\theta(x)$
- 生成式：VAE / GAN / Flow / Diffusion 等 $p_\theta(x)$ 或 $p_\theta(x\mid c)$
- 对比 / 多模态：如 CLIP 风格的双塔 encoder

本质：选“用什么网络、什么计算图、什么训练范式”来逼近 Step 2 中的优化目标。

---

### Step 4 — 用梯度下降训练出参数（Training）

在训练数据上，用梯度法优化损失，得到参数 $\hat\theta$：

- 基本形式：$\theta \leftarrow \theta - \eta \nabla_\theta \ell(\theta; x)$
- 实际使用：SGD / Momentum / Adam / AdamW 等

训练完成后得到具体模型 $f_{\hat\theta}$ 或 $p_{\hat\theta}$。

---

### Step 5 — 测试与推断（Test / Inference）

将训练好的模型应用到实际问题中：

- 判别模型：对新输入 $x$ 输出预测标签 $\hat y = f_{\hat\theta}(x)$
- 生成模型：从 $p_{\hat\theta}(x)$ 采样生成新样本
- 自监督模型：输出表示 $z = f_{\hat\theta}(x)$ 供下游任务使用
- 强化学习：用策略 $\pi_{\hat\theta}$ 与环境交互执行决策

这一步是“模型落地”和“性能评估”。
